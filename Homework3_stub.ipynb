{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "301bf995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "from sklearn import svm\n",
    "import numpy\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "31cab31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "33f967ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3e25a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f88efc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        u,b,r = l.strip().split(',')\n",
    "        r = int(r)\n",
    "        yield u,b,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3a5f39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b3b16eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data structures that will be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "09ac1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "allRatings = []\n",
    "userRatings = defaultdict(list)\n",
    "\n",
    "for user,book,r in readCSV(\"assignment1/train_Interactions.csv.gz\"):\n",
    "  r = int(r)\n",
    "  allRatings.append(r)\n",
    "  userRatings[user].append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e4717806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d296d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "allInteractions = [(user, book, int(r)) for user, book, r in readCSV(\"assignment1/train_Interactions.csv.gz\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ed93d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "globalAverage = sum(allRatings) / len(allRatings)\n",
    "userAverage = {}\n",
    "for u in userRatings:\n",
    "  userAverage[u] = sum(userRatings[u]) / len(userRatings[u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ca3c2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsTrain = allInteractions[:190000]  # Use tuples of (user, book, rating)\n",
    "ratingsValid = allInteractions[190000:]  # Validation set with tuples of (user, book, rating)\n",
    "\n",
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "for u, b, r in ratingsTrain:  # Unpack the tuples properly\n",
    "    ratingsPerUser[u].append((b, r))  # Append book-rating pairs\n",
    "    ratingsPerItem[b].append((u, r))  # Append user-rating pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f84a64af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allRatings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd83d3",
   "metadata": {},
   "source": [
    "### Tasks (Read prediction)\n",
    "Since we don’t have access to the test labels, we’ll need to simulate validation/test sets of our own.\n",
    "So, let’s split the training data (‘train Interactions.csv.gz’) as follows:\n",
    "(1) Reviews 1-190,000 for training\n",
    "(2) Reviews 190,001-200,000 for validation\n",
    "(3) Upload only when you have a good model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fce25d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Read prediction                                #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "abb17ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from baseline code\n",
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for user,book,_ in readCSV(\"assignment1/train_Interactions.csv.gz\"):\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalRead/2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2dcfd5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sets of users who have read each book\n",
    "usersPerItem = defaultdict(set)\n",
    "for u, b, _ in ratingsTrain:\n",
    "    usersPerItem[b].add(u)\n",
    "\n",
    "# Create a dictionary with all books read by each user in the training data\n",
    "itemsPerUser = defaultdict(set)\n",
    "for u, b, _ in ratingsTrain:\n",
    "    itemsPerUser[u].add(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b256c9",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Although we have built a validation set, it only consists of positive samples. For this task we also need\n",
    "examples of user/item pairs that weren’t read. For each (user,book) entry in the validation set, sample a\n",
    "negative entry by randomly choosing a book that user hasn’t read. Evaluate the performance (accuracy)\n",
    "of the baseline model on the validation set you have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "80f40789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation set with negative samples\n",
    "validation_with_negatives = []\n",
    "for u, b, r in ratingsValid:\n",
    "    # Positive sample (user has read this book)\n",
    "    validation_with_negatives.append((u, b, 1))  \n",
    "    \n",
    "    # Negative sample (user hasn't read this book)\n",
    "    while True:\n",
    "        negative_book = random.choice(list(bookCount.keys()))\n",
    "        if negative_book not in itemsPerUser[u]:\n",
    "            validation_with_negatives.append((u, negative_book, 0))\n",
    "            break\n",
    "# Baseline prediction based on popularity\n",
    "def baseline_predict(user, book):\n",
    "    return 1 if book in return1 else 0\n",
    "\n",
    "# Calculate accuracy as a float value for baseline model on validation set\n",
    "correct_predictions = 0\n",
    "total_predictions = len(validation_with_negatives)\n",
    "\n",
    "for u, b, actual in validation_with_negatives:\n",
    "    prediction = baseline_predict(u, b)\n",
    "    if prediction == actual:\n",
    "        correct_predictions += 1\n",
    "\n",
    "acc1 = correct_predictions / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8af7b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q1'] = acc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cd53d38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71285"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6839df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a76a17a",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "The existing ‘read prediction’ baseline just returns True if the item in question is ‘popular,’ using a\n",
    "threshold based on those books which account for 50% of all interactions (totalRead/2). Assuming that\n",
    "the ‘non-read’ test examples are a random sample of user-book pairs, this threshold may not be the best\n",
    "one. See if you can find a better threshold (or otherwise modify the threshold strategy); report the new threshold and its performance of your improved model on your validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "50491907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible thresholds (percentages of total reads that books need to reach)\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "best_threshold = 0.5  # Start with the initial threshold\n",
    "best_accuracy = 0\n",
    "\n",
    "# Iterate through each threshold and evaluate performance\n",
    "for threshold in thresholds:\n",
    "    # Calculate the interaction count required for this threshold\n",
    "    count_threshold = totalRead * threshold\n",
    "\n",
    "    # Update return1 to include books that meet the threshold\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > count_threshold:\n",
    "            break\n",
    "\n",
    "    # Calculate accuracy with this threshold\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(validation_with_negatives)\n",
    "\n",
    "    for u, b, actual in validation_with_negatives:\n",
    "        prediction = baseline_predict(u, b)\n",
    "        if prediction == actual:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # Update best threshold and accuracy if this threshold is better\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_threshold = threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "263c16a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q2'] = [best_threshold, best_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7cf81484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7, 0.7535]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[best_threshold, best_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fcb6b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q2'][0])\n",
    "assertFloat(answers['Q2'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddfe3f1",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "A stronger baseline (This baseline is not always stronger, depending on the dataset.) \n",
    "than the one provided might make use of the Jaccard similarity (or another similarity\n",
    "metric). Given a pair (u, b) in the validation set, consider all training items b′that user u has read. \n",
    "For each, compute the Jaccard similarity between b and b′, \n",
    "i.e., users (in the training set) who have read b and users who have read b′. \n",
    "Predict as ‘read’ if the maximum of these Jaccard similarities exceeds a threshold \n",
    "(you may choose the threshold that works best). Report the performance on your validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "04a6f2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Popularity Threshold: 0.5, Best Similarity Threshold: 0.1, Best Accuracy: 0.71305\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Cosine similarity calculation based on co-occurrence\n",
    "def cosine_similarity(set1, set2):\n",
    "    intersection_size = len(set1 & set2)\n",
    "    norm1 = math.sqrt(len(set1))\n",
    "    norm2 = math.sqrt(len(set2))\n",
    "    return intersection_size / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0\n",
    "\n",
    "# Combined prediction function using popularity and similarity\n",
    "def combined_predict(user, book, popularity_threshold, similarity_threshold):\n",
    "    # Step 1: Check if the book is popular enough\n",
    "    if book in return1_popular_books:  # Use popular books set\n",
    "        return 1  # Predict \"read\" if the book is popular\n",
    "\n",
    "    # Step 2: Check similarity with books the user has read\n",
    "    user_books = itemsPerUser.get(user, set())\n",
    "    max_similarity = 0\n",
    "    \n",
    "    for b_prime in user_books:\n",
    "        similarity = cosine_similarity(usersPerItem.get(book, set()), usersPerItem.get(b_prime, set()))\n",
    "        max_similarity = max(max_similarity, similarity)\n",
    "        \n",
    "    # Predict \"read\" if the maximum similarity exceeds the similarity threshold\n",
    "    return 1 if max_similarity >= similarity_threshold else 0\n",
    "\n",
    "# Define thresholds to test for both popularity and similarity\n",
    "popularity_thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]  # Test various percentages\n",
    "similarity_thresholds = [0.05, 0.1, 0.15, 0.2, 0.25]  # Finer increments for similarity\n",
    "\n",
    "best_accuracy = 0\n",
    "best_popularity_threshold = 0\n",
    "best_similarity_threshold = 0\n",
    "\n",
    "# Evaluate performance by iterating over popularity and similarity thresholds\n",
    "for popularity_threshold in popularity_thresholds:\n",
    "    # Determine popular books based on the current popularity threshold\n",
    "    count_threshold = totalRead * popularity_threshold\n",
    "    return1_popular_books = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1_popular_books.add(i)\n",
    "        if count > count_threshold:\n",
    "            break\n",
    "    \n",
    "    for similarity_threshold in similarity_thresholds:\n",
    "        correct_predictions = 0\n",
    "        total_predictions = len(validation_with_negatives)\n",
    "        \n",
    "        # Evaluate each validation pair using the combined model\n",
    "        for u, b, actual in validation_with_negatives:\n",
    "            prediction = combined_predict(u, b, popularity_threshold, similarity_threshold)\n",
    "            if prediction == actual:\n",
    "                correct_predictions += 1\n",
    "        \n",
    "        # Calculate accuracy for the current threshold combination\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        # Update best thresholds if current combination performs better\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_popularity_threshold = popularity_threshold\n",
    "            best_similarity_threshold = similarity_threshold\n",
    "\n",
    "# Output the best accuracy and threshold combination\n",
    "acc3 = best_accuracy\n",
    "print(f\"Best Popularity Threshold: {best_popularity_threshold}, Best Similarity Threshold: {best_similarity_threshold}, Best Accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "96916c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71305"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc3 = best_accuracy \n",
    "acc3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c9f42",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Improve the above predictor by incorporating both a Jaccard-based threshold and a popularity based\n",
    "threshold. Report the performance on your validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "826e15c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Jaccard similarity function\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Step 1: Find the best Jaccard threshold first (from Question 3)\n",
    "best_jaccard_threshold = 0.3  # Assume this was optimal from Q3\n",
    "\n",
    "# Step 2: Optimize popularity threshold only, given the best Jaccard threshold\n",
    "best_combined_accuracy = 0\n",
    "best_popularity_threshold = 0.5  # Starting point for popularity threshold\n",
    "\n",
    "# Smaller set of popularity thresholds for quicker testing\n",
    "popularity_thresholds = [0.3, 0.5, 0.7]  # Adjust as needed\n",
    "\n",
    "# Memoization dictionary for Jaccard similarities\n",
    "jaccard_cache = {}\n",
    "\n",
    "for p_threshold in popularity_thresholds:\n",
    "    # Calculate popularity set for the given threshold\n",
    "    count_threshold = totalRead * p_threshold\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > count_threshold:\n",
    "            break\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(validation_with_negatives)\n",
    "\n",
    "    # Predict with combined Jaccard and popularity threshold\n",
    "    for u, b, actual in validation_with_negatives:\n",
    "        # Check Jaccard-based prediction\n",
    "        if u not in itemsPerUser or not itemsPerUser[u]:\n",
    "            jaccard_prediction = 0  # No history for user\n",
    "        else:\n",
    "            max_similarity = max(\n",
    "                jaccard_cache.setdefault(\n",
    "                    (b, b_prime), jaccard_similarity(usersPerItem[b], usersPerItem[b_prime])\n",
    "                )\n",
    "                for b_prime in itemsPerUser[u]\n",
    "            )\n",
    "            jaccard_prediction = 1 if max_similarity > best_jaccard_threshold else 0\n",
    "\n",
    "        # Check popularity-based prediction\n",
    "        popularity_prediction = 1 if b in return1 else 0\n",
    "\n",
    "        # Combined prediction (logical OR)\n",
    "        prediction = 1 if jaccard_prediction == 1 or popularity_prediction == 1 else 0\n",
    "\n",
    "        # Compare to actual label\n",
    "        if prediction == actual:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    if accuracy > best_combined_accuracy:\n",
    "        best_combined_accuracy = accuracy\n",
    "        best_popularity_threshold = p_threshold\n",
    "\n",
    "acc4 = best_combined_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0f528938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7535"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "83ab0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3'] = acc3\n",
    "answers['Q4'] = acc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fbdd0c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q3'])\n",
    "assertFloat(answers['Q4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ebe3b",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "To run our model on the test set, we’ll have to use the files ‘pairs Read.csv’ to find the userID/bookID\n",
    "pairs about which we have to make predictions. Using that data, run the above model and upload your\n",
    "solution to the (Assignment 1) Gradescope. If you’ve already uploaded a better solution, that’s fine\n",
    "too! Your answer should be the string “I confirm that I have uploaded an assignment submission to\n",
    "gradescope”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Read.csv\", 'w')\n",
    "for l in open(\"assignment1/pairs_Read.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split(',')\n",
    "    # (etc.)\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "297b5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5'] = \"I confirm that I have uploaded an assignment submission to gradescope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b3cb95e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(answers['Q5']) == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bcf70975",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Rating prediction                              \n",
    "# Let’s start by building our training/validation sets much as we did for the first task. This time building a\n",
    "# validation set is more straightforward: you can simply use part of the data for validation, and do not need to\n",
    "# randomly sample non-read users/books\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7829b7",
   "metadata": {},
   "source": [
    "•\tFor Question 6, we are tasked with using a fixed regularization parameter \\lambda = 1 and minimizing the Sum of Squared Errors (SSE), including a regularization term for user and item biases.\n",
    "•\tThe goal here is to report the MSE on the validation set after training the model with \\lambda = 1.\n",
    "\n",
    "•\tIn Question 8, we are tuning \\lambda by trying multiple values (e.g., [0.01, 0.1, 1, 10, 100]) and finding the one that yields the best performance on the validation set.\n",
    "•\tWe then report the optimal \\lambda, along with the MSE for that \\lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b960a7",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Fit a predictor of the form\n",
    "rating(user, item) ≃ α + βuser + βitem,\n",
    "by fitting the mean and the two bias terms as described in the lecture notes. Use a regularization\n",
    "parameter of λ = 1. Report the MSE on the validation set.\n",
    "For this question note carefully that the objective optimized should be the sum of squared errors plus\n",
    "the regularizer (squared ℓ2 norm scaled by λ), i.e., it should not be the mean squared error. Although\n",
    "using the MSE vs SSE is equivalent (up to a change in λ) it is important that your objective follows this\n",
    "exact specification so that everyone’s solution is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6d69e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize parameters for the model\n",
    "alpha = 0  # Global bias\n",
    "user_bias = defaultdict(float)  # β_user\n",
    "item_bias = defaultdict(float)  # β_item\n",
    "lambda_reg = 1  # Regularization parameter\n",
    "\n",
    "# Derive unique user and book sets from ratingsTrain\n",
    "train_user_ids = set(u for u, _, _ in ratingsTrain)\n",
    "train_book_ids = set(b for _, b, _ in ratingsTrain)\n",
    "\n",
    "# Objective function: Sum of Squared Errors (SSE) with regularization\n",
    "def train_bias_model_fixed_lambda(user_ids, item_ids, ratings, iterations=10, learning_rate=0.01, lambda_reg=1):\n",
    "    global alpha, user_bias, item_bias\n",
    "    alpha = 1.0  # Initialize global bias\n",
    "    user_bias = defaultdict(float)  # Reset user biases\n",
    "    item_bias = defaultdict(float)  # Reset item biases\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        for u, i, r in ratings:\n",
    "            # Predicted rating based on current parameters\n",
    "            pred = alpha + user_bias[u] + item_bias[i]\n",
    "            \n",
    "            # Calculate error for this prediction\n",
    "            error = r - pred  # Target is 1 for \"read\" status\n",
    "            \n",
    "            # Update parameters with regularization for SSE\n",
    "            alpha += learning_rate * error\n",
    "            user_bias[u] += learning_rate * (error - lambda_reg * user_bias[u])\n",
    "            item_bias[i] += learning_rate * (error - lambda_reg * item_bias[i])\n",
    "\n",
    "# Train the model on the training data with λ = 1\n",
    "train_bias_model_fixed_lambda(train_user_ids, train_book_ids, ratingsTrain, lambda_reg=lambda_reg)\n",
    "\n",
    "# Calculate MSE on the validation set\n",
    "def calculate_mse(validation_ratings):\n",
    "    mse_total = 0\n",
    "    count = 0\n",
    "    for u, i, r in validation_ratings:\n",
    "        # Predicted rating\n",
    "        pred = alpha + user_bias[u] + item_bias[i]\n",
    "        \n",
    "        # Compute squared error\n",
    "        mse_total += (r - pred) ** 2\n",
    "        count += 1\n",
    "    return mse_total / count if count > 0 else float('nan')\n",
    "\n",
    "# Store the MSE for the validation set for Question 6\n",
    "validMSE = calculate_mse(ratingsValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "422ab930",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6'] = validMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5509bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a572a80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.488958799100907"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9826cdc9",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Report the user IDs that have the largest and smallest (i.e., largest negative) values of βu, along with\n",
    "the beta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c0f9a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that Question 6 has been run to compute user biases\n",
    "# After training the bias model, extract user bias values for analysis\n",
    "\n",
    "# Get the user bias values (from the model trained in Q6)\n",
    "user_bias_values = [(user, beta) for user, beta in user_bias.items()]\n",
    "\n",
    "# Find the user with the largest positive and largest negative biases\n",
    "maxUser, maxBeta = max(user_bias_values, key=lambda x: x[1])  # User with highest positive bias\n",
    "minUser, minBeta = min(user_bias_values, key=lambda x: x[1])  # User with largest negative bias\n",
    "\n",
    "# Ensure user IDs are converted to strings and biases are floats\n",
    "maxUser = str(maxUser)\n",
    "minUser = str(minUser)\n",
    "maxBeta = float(maxBeta)\n",
    "minBeta = float(minBeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c61b675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q7'] = [maxUser, minUser, maxBeta, minBeta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7aca2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [type(x) for x in answers['Q7']] == [str, str, float, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "70255704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u79275096', 'u88024921', 0.6925560635645533, -1.8178145604516576]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[maxUser, minUser, maxBeta, minBeta]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a416949",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Find a better value of λ using your validation set. Report the value you chose, its MSE, and upload your\n",
    "solution to gradescope by running it on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ae54cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of lambda values to test\n",
    "lambda_values = [0.01, 0.1, 1, 10, 100]\n",
    "best_lambda = None\n",
    "best_mse = float('inf')\n",
    "\n",
    "# Function to train the bias model with a given lambda\n",
    "def train_bias_model_with_lambda(user_ids, item_ids, ratings, lambda_reg, iterations=10, learning_rate=0.01):\n",
    "    global alpha, user_bias, item_bias\n",
    "    alpha = 1.0  # Reset global average\n",
    "    user_bias = defaultdict(float)  # Reset user biases\n",
    "    item_bias = defaultdict(float)  # Reset item biases\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        for u, i, r in ratings:\n",
    "            # Predicted rating based on current parameters\n",
    "            pred = alpha + user_bias[u] + item_bias[i]\n",
    "            \n",
    "            # Calculate error\n",
    "            error = r - pred  # Target is 1 for \"read\" status\n",
    "            \n",
    "            # Update parameters with regularization\n",
    "            alpha += learning_rate * (error - lambda_reg * alpha)\n",
    "            user_bias[u] += learning_rate * (error - lambda_reg * user_bias[u])\n",
    "            item_bias[i] += learning_rate * (error - lambda_reg * item_bias[i])\n",
    "\n",
    "# Train and evaluate the model for each lambda\n",
    "for lambda_reg in lambda_values:\n",
    "    # Train the model with the current lambda\n",
    "    train_bias_model_with_lambda(train_user_ids, train_book_ids, ratingsTrain, lambda_reg)\n",
    "    \n",
    "    # Calculate MSE on the validation set\n",
    "    mse = calculate_mse(ratingsValid)\n",
    "    \n",
    "    # Check if this is the best lambda so far\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_lambda = lambda_reg\n",
    "\n",
    "# Ensure lambda and MSE are stored as floats\n",
    "lamb = float(best_lambda)\n",
    "validMSE = float(best_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f1880fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q8'] = (lamb, validMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "56b09160",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q8'][0])\n",
    "assertFloat(answers['Q8'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1b07045f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01, 1.449611497627705)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamb, validMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b9bd53b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Rating.csv\", 'w')\n",
    "for l in open(\"assignment1/pairs_Rating.csv\"):\n",
    "  if l.startswith(\"userID\"):\n",
    "    #header\n",
    "    predictions.write(l)\n",
    "    continue\n",
    "  u,b = l.strip().split(',')\n",
    "  if u in userAverage:\n",
    "    predictions.write(u + ',' + b + ',' + str(userAverage[u]) + '\\n')\n",
    "  else:\n",
    "    predictions.write(u + ',' + b + ',' + str(globalAverage) + '\\n')\n",
    "    \n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3000bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw1.txt\", 'w') # Write your answers to a file\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
